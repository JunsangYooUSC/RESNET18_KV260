{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viCeY2dYQtYl",
        "outputId": "53fab968-d2fe-4678-ee10-980d465cb196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDIWwTC5QaPV",
        "outputId": "5358fd2e-736e-4aa9-c8a0-03416906bc41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available and being used.\n",
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "import random\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision.transforms import Resize\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch.quantization\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# use GPU if available\n",
        "if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(\"GPU is available and being used.\")\n",
        "else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"GPU is not available, using CPU instead.\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Block and ResNet18 Architecture"
      ],
      "metadata": {
        "id": "vTJyc_IB-OYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1  # No expansion in BasicBlock\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.stride = stride\n",
        "\n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels, out_channels,\n",
        "            kernel_size=kernel_size, stride=stride, padding=padding, bias=False\n",
        "        )\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            out_channels, out_channels,\n",
        "            kernel_size=kernel_size, stride=1, padding=padding, bias=False\n",
        "        )\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Downsample layer for shortcut connection (if needed)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x  # Save the input tensor for the shortcut\n",
        "\n",
        "        # First layer\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # Second layer\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        # Apply downsampling to the identity if necessary\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        # Add the identity (shortcut connection)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "oGCjyexhqhZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(ResNet18, self).__init__()\n",
        "\n",
        "        # Initial Convolution and Max Pool\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=3, out_channels=64,\n",
        "            kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Define layers using your BasicBlock\n",
        "        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n",
        "\n",
        "\n",
        "        # Adaptive Average Pooling\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
        "        downsample = None\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(BasicBlock(in_channels, out_channels, stride=stride, downsample=downsample))\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(BasicBlock(out_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)"
      ],
      "metadata": {
        "id": "Lzxx8P51qUoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization"
      ],
      "metadata": {
        "id": "djL_Dv4U-VLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters:\n",
        "learning_rate = 0.0005\n",
        "momentum = 0.9\n",
        "weight_decay = 1e-3\n",
        "\n",
        "num_epochs = 6\n",
        "T_max = num_epochs\n",
        "eta_min = 1e-5\n"
      ],
      "metadata": {
        "id": "URjpfX0J96rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations for CIFAR-100 dataset\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Download the CIFAR-100 training dataset\n",
        "download_train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "download_test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "batch_size = 64\n",
        "# Create DataLoader for training and validation datasets\n",
        "train_loader = DataLoader(download_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(download_test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnlUgWBP-Hpl",
        "outputId": "a04431e2-d947-4e47-9b63-c80b06d95b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ],
      "metadata": {
        "id": "mZNxTN0eCRbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save/Load function"
      ],
      "metadata": {
        "id": "Bdlts3cdALyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_checkpoint(model, optimizer, path):\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    model.to(device)\n",
        "\n",
        "    for state in optimizer.state.values():\n",
        "        for k, v in state.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                state[k] = v.to(device)\n",
        "    return model, optimizer, epoch"
      ],
      "metadata": {
        "id": "ypPp3eIJ_0Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, epoch, path):\n",
        "    # Create the directory if it doesn't exist\n",
        "    import os\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, path)"
      ],
      "metadata": {
        "id": "Mpw1_h8bSA5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation Function"
      ],
      "metadata": {
        "id": "3QAndPpwBEyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "TWBCpQk_Qz69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quantization Function"
      ],
      "metadata": {
        "id": "DdvwcTwFAv95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fixed_point_quantize_weights(weights, total_bits, int_bits):\n",
        "    frac_bits = total_bits - int_bits\n",
        "    delta = 2 ** (-frac_bits)\n",
        "    max_val = (2 ** (total_bits - 1) - 1) * delta\n",
        "    min_val = -2 ** (total_bits - 1) * delta\n",
        "\n",
        "    q_weights = torch.clamp(torch.round(weights / delta), min_val / delta, max_val / delta) * delta\n",
        "    return q_weights"
      ],
      "metadata": {
        "id": "BUdFcV6WR-GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedConv2d(nn.Conv2d):\n",
        "    def __init__(self, *args, total_bits=8, weight_int_bits=2, input_int_bits=2, **kwargs):\n",
        "        super(QuantizedConv2d, self).__init__(*args, **kwargs)\n",
        "        self.total_bits = total_bits\n",
        "        self.weight_int_bits = weight_int_bits\n",
        "        self.input_int_bits = input_int_bits\n",
        "    def forward(self, input):\n",
        "        # quantize input\n",
        "        quantized_input = fixed_point_quantize_weights(input, self.total_bits, self.input_int_bits)\n",
        "        # quantize weights\n",
        "        original_weights = self.weight.data\n",
        "        quantized_weights = fixed_point_quantize_weights(original_weights, self.total_bits, self.weight_int_bits)\n",
        "        output = F.conv2d(quantized_input, quantized_weights, self.bias, self.stride,\n",
        "                          self.padding, self.dilation, self.groups)\n",
        "        return output"
      ],
      "metadata": {
        "id": "Mis65zuZSgWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## quantize conv\n",
        "def quantize_conv2d(model, total_bits, weight_int_bits, input_int_bits):\n",
        "    for name, m in model.named_children():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            new_layer = QuantizedConv2d(\n",
        "                in_channels=m.in_channels,\n",
        "                out_channels=m.out_channels,\n",
        "                kernel_size=m.kernel_size,\n",
        "                stride=m.stride,\n",
        "                padding=m.padding,\n",
        "                dilation=m.dilation,\n",
        "                groups=m.groups,\n",
        "                bias=(m.bias is not None),\n",
        "                total_bits=total_bits,\n",
        "                weight_int_bits=weight_int_bits,\n",
        "                input_int_bits=input_int_bits\n",
        "            )\n",
        "            new_layer.weight.data = fixed_point_quantize_weights(m.weight.data.clone(), total_bits, weight_int_bits)\n",
        "            if m.bias is not None:\n",
        "                new_layer.bias.data = fixed_point_quantize_weights(m.bias.data.clone(), total_bits, weight_int_bits)\n",
        "\n",
        "            setattr(model, name, new_layer)\n",
        "        elif len(list(m.children())) > 0:\n",
        "            quantize_conv2d(m, total_bits, weight_int_bits, input_int_bits)"
      ],
      "metadata": {
        "id": "8fd1vqhQTWll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reload from our trained model (ResNet18) as starting point, start_epoch will be initialized as 0"
      ],
      "metadata": {
        "id": "nEjCaH8n_CCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = '/content/drive/My Drive/Colab Notebooks/checkpoints/transfer_learning_checkpoint.pth'\n",
        "model = ResNet18(num_classes=100)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "model, optimizer, start_epoch = load_checkpoint(model, optimizer, checkpoint_path)\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Create model, schedueler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n",
        "\n",
        "# Verify the model\n",
        "print(\"start epoch: \", start_epoch)\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data.shape, param.data.sum())\n",
        "\n",
        "start_epoch = 0 # initialize start_epoch for new training quantized ResNet18\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNAyDbF5-roR",
        "outputId": "7e4383c7-3b4c-4dd9-c49c-1db5f051179e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-919c0df432fe>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(path, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start epoch:  60\n",
            "conv1.weight torch.Size([64, 3, 7, 7]) tensor(0.2768, device='cuda:0')\n",
            "bn1.weight torch.Size([64]) tensor(16.4849, device='cuda:0')\n",
            "bn1.bias torch.Size([64]) tensor(11.5917, device='cuda:0')\n",
            "layer1.0.conv1.weight torch.Size([64, 64, 3, 3]) tensor(-113.8010, device='cuda:0')\n",
            "layer1.0.bn1.weight torch.Size([64]) tensor(21.7344, device='cuda:0')\n",
            "layer1.0.bn1.bias torch.Size([64]) tensor(-2.1847, device='cuda:0')\n",
            "layer1.0.conv2.weight torch.Size([64, 64, 3, 3]) tensor(-32.7843, device='cuda:0')\n",
            "layer1.0.bn2.weight torch.Size([64]) tensor(21.3155, device='cuda:0')\n",
            "layer1.0.bn2.bias torch.Size([64]) tensor(0.2216, device='cuda:0')\n",
            "layer1.1.conv1.weight torch.Size([64, 64, 3, 3]) tensor(-89.2124, device='cuda:0')\n",
            "layer1.1.bn1.weight torch.Size([64]) tensor(21.0363, device='cuda:0')\n",
            "layer1.1.bn1.bias torch.Size([64]) tensor(-5.3487, device='cuda:0')\n",
            "layer1.1.conv2.weight torch.Size([64, 64, 3, 3]) tensor(-46.4579, device='cuda:0')\n",
            "layer1.1.bn2.weight torch.Size([64]) tensor(25.1155, device='cuda:0')\n",
            "layer1.1.bn2.bias torch.Size([64]) tensor(-1.9190, device='cuda:0')\n",
            "layer2.0.conv1.weight torch.Size([128, 64, 3, 3]) tensor(-107.1819, device='cuda:0')\n",
            "layer2.0.bn1.weight torch.Size([128]) tensor(40.5017, device='cuda:0')\n",
            "layer2.0.bn1.bias torch.Size([128]) tensor(-8.6203, device='cuda:0')\n",
            "layer2.0.conv2.weight torch.Size([128, 128, 3, 3]) tensor(-183.9758, device='cuda:0')\n",
            "layer2.0.bn2.weight torch.Size([128]) tensor(41.9293, device='cuda:0')\n",
            "layer2.0.bn2.bias torch.Size([128]) tensor(-0.4551, device='cuda:0')\n",
            "layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1]) tensor(-21.1976, device='cuda:0')\n",
            "layer2.0.downsample.1.weight torch.Size([128]) tensor(24.9709, device='cuda:0')\n",
            "layer2.0.downsample.1.bias torch.Size([128]) tensor(-0.4551, device='cuda:0')\n",
            "layer2.1.conv1.weight torch.Size([128, 128, 3, 3]) tensor(-225.6396, device='cuda:0')\n",
            "layer2.1.bn1.weight torch.Size([128]) tensor(41.1218, device='cuda:0')\n",
            "layer2.1.bn1.bias torch.Size([128]) tensor(-26.9120, device='cuda:0')\n",
            "layer2.1.conv2.weight torch.Size([128, 128, 3, 3]) tensor(-187.6203, device='cuda:0')\n",
            "layer2.1.bn2.weight torch.Size([128]) tensor(36.2126, device='cuda:0')\n",
            "layer2.1.bn2.bias torch.Size([128]) tensor(-19.3644, device='cuda:0')\n",
            "layer3.0.conv1.weight torch.Size([256, 128, 3, 3]) tensor(-205.2594, device='cuda:0')\n",
            "layer3.0.bn1.weight torch.Size([256]) tensor(67.0644, device='cuda:0')\n",
            "layer3.0.bn1.bias torch.Size([256]) tensor(-27.8942, device='cuda:0')\n",
            "layer3.0.conv2.weight torch.Size([256, 256, 3, 3]) tensor(-223.3968, device='cuda:0')\n",
            "layer3.0.bn2.weight torch.Size([256]) tensor(70.7153, device='cuda:0')\n",
            "layer3.0.bn2.bias torch.Size([256]) tensor(-9.0821, device='cuda:0')\n",
            "layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1]) tensor(-17.3523, device='cuda:0')\n",
            "layer3.0.downsample.1.weight torch.Size([256]) tensor(18.0748, device='cuda:0')\n",
            "layer3.0.downsample.1.bias torch.Size([256]) tensor(-9.0821, device='cuda:0')\n",
            "layer3.1.conv1.weight torch.Size([256, 256, 3, 3]) tensor(-487.6661, device='cuda:0')\n",
            "layer3.1.bn1.weight torch.Size([256]) tensor(60.0683, device='cuda:0')\n",
            "layer3.1.bn1.bias torch.Size([256]) tensor(-55.1665, device='cuda:0')\n",
            "layer3.1.conv2.weight torch.Size([256, 256, 3, 3]) tensor(-491.3675, device='cuda:0')\n",
            "layer3.1.bn2.weight torch.Size([256]) tensor(51.0004, device='cuda:0')\n",
            "layer3.1.bn2.bias torch.Size([256]) tensor(-35.2249, device='cuda:0')\n",
            "layer4.0.conv1.weight torch.Size([512, 256, 3, 3]) tensor(-267.2814, device='cuda:0')\n",
            "layer4.0.bn1.weight torch.Size([512]) tensor(37.6899, device='cuda:0')\n",
            "layer4.0.bn1.bias torch.Size([512]) tensor(-46.4085, device='cuda:0')\n",
            "layer4.0.conv2.weight torch.Size([512, 512, 3, 3]) tensor(-438.4600, device='cuda:0')\n",
            "layer4.0.bn2.weight torch.Size([512]) tensor(51.6485, device='cuda:0')\n",
            "layer4.0.bn2.bias torch.Size([512]) tensor(-43.0391, device='cuda:0')\n",
            "layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1]) tensor(-58.6325, device='cuda:0')\n",
            "layer4.0.downsample.1.weight torch.Size([512]) tensor(38.3615, device='cuda:0')\n",
            "layer4.0.downsample.1.bias torch.Size([512]) tensor(-43.0391, device='cuda:0')\n",
            "layer4.1.conv1.weight torch.Size([512, 512, 3, 3]) tensor(-1060.5111, device='cuda:0')\n",
            "layer4.1.bn1.weight torch.Size([512]) tensor(32.5491, device='cuda:0')\n",
            "layer4.1.bn1.bias torch.Size([512]) tensor(-33.5614, device='cuda:0')\n",
            "layer4.1.conv2.weight torch.Size([512, 512, 3, 3]) tensor(-44.4516, device='cuda:0')\n",
            "layer4.1.bn2.weight torch.Size([512]) tensor(329.4836, device='cuda:0')\n",
            "layer4.1.bn2.bias torch.Size([512]) tensor(121.0225, device='cuda:0')\n",
            "fc.weight torch.Size([100, 512]) tensor(-36.5052, device='cuda:0')\n",
            "fc.bias torch.Size([100]) tensor(0.0090, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load from quantized checkpoint (ResNet18) as starting point, start_epoch will be previously saved checkpoint"
      ],
      "metadata": {
        "id": "BfrI-e8H_4oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = '/content/drive/My Drive/Colab Notebooks/checkpoints/quantized_checkpoint.pth'\n",
        "model = ResNet18(num_classes=100)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "model, optimizer, start_epoch = load_checkpoint(model, optimizer, checkpoint_path)\n",
        "model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Create model, schedueler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n",
        "\n",
        "# Verify the model\n",
        "print(\"start epoch: \", start_epoch)\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data.shape, param.data.sum())"
      ],
      "metadata": {
        "id": "BQMGOWD2AdAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantized Model Search Space"
      ],
      "metadata": {
        "id": "AyJ0P0PVrh11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_accuracy = 0\n",
        "best_weight_int_bits = 0\n",
        "best_input_int_bits = 0\n",
        "weight_int_bits_options = [2, 3, 4]\n",
        "input_int_bits_options = [2, 3, 4]\n",
        "\n",
        "for weight_int_bits in weight_int_bits_options:\n",
        "    for input_int_bits in input_int_bits_options:\n",
        "        # Create a copy of the model for quantization\n",
        "        quantized_model = copy.deepcopy(model)\n",
        "\n",
        "        # Quantize the model\n",
        "        quantize_conv2d(quantized_model, 8, weight_int_bits, input_int_bits)\n",
        "        quantized_model = quantized_model.to(device)\n",
        "\n",
        "        # Evaluate the quantized model\n",
        "        accuracy = evaluate(quantized_model, test_loader, device)\n",
        "\n",
        "        print(f\"Weight int bits: {weight_int_bits}, Input int bits: {input_int_bits}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_weight_int_bits = weight_int_bits\n",
        "            best_input_int_bits = input_int_bits\n",
        "\n",
        "print(f\"Best weight int bits: {best_weight_int_bits}, Best input int bits: {best_input_int_bits}, Best Accuracy: {best_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wzE5wUWqkKP",
        "outputId": "0c86a2b8-2c06-41e4-a972-527d9a90113c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight int bits: 2, Input int bits: 2, Accuracy: 67.22%\n",
            "Weight int bits: 2, Input int bits: 3, Accuracy: 67.40%\n",
            "Weight int bits: 2, Input int bits: 4, Accuracy: 66.90%\n",
            "Weight int bits: 3, Input int bits: 2, Accuracy: 42.44%\n",
            "Weight int bits: 3, Input int bits: 3, Accuracy: 41.84%\n",
            "Weight int bits: 3, Input int bits: 4, Accuracy: 39.53%\n",
            "Weight int bits: 4, Input int bits: 2, Accuracy: 1.11%\n",
            "Weight int bits: 4, Input int bits: 3, Accuracy: 1.06%\n",
            "Weight int bits: 4, Input int bits: 4, Accuracy: 1.04%\n",
            "Best weight int bits: 2, Best input int bits: 3, Best Accuracy: 67.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantized Model and train with int_bit [2, 3]"
      ],
      "metadata": {
        "id": "hgyAZX_yBLNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = copy.deepcopy(model)\n",
        "\n",
        "weight_int_bits = 2\n",
        "input_int_bits = 3\n",
        "\n",
        "quantize_conv2d(quantized_model, 8, weight_int_bits, input_int_bits)\n",
        "quantized_model = quantized_model.to(device)"
      ],
      "metadata": {
        "id": "bBdcdzI3SjFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test Quantization model"
      ],
      "metadata": {
        "id": "xGlTkLZLBN4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## quantize test 0\n",
        "a = torch.arange(100)/10-5\n",
        "aa = fixed_point_quantize_weights(a, 8, 2)\n",
        "print(f\"a: {a}\")\n",
        "print(f\"aa: {aa}\")\n",
        "diff_a = a-aa\n",
        "print(f\"diff_a.std: {diff_a.std()}\")\n",
        "print(f\"diff_a.rms_mean: {torch.mean(torch.sqrt(diff_a**2))}\")\n",
        "\n",
        "## quantize test\n",
        "with torch.no_grad():\n",
        "    img = (torch.rand((1,3,224,224))-0.5).to(device)\n",
        "    X = model.conv1(img)\n",
        "    print(\"X: \", X[0][0])\n",
        "    Xq = quantized_model.conv1(img)\n",
        "    print(\"Xq: \", Xq[0][0])\n",
        "    diff_X = X-Xq\n",
        "    print(f\"diff_X.std: {diff_X.std()}\")\n",
        "    print(f\"diff_X.rms_mean: {torch.mean(torch.sqrt(diff_X**2))}\")\n",
        "    print(f\"diff_X.max: {diff_X.max()}\")\n",
        "    layer = model.layer1[0]\n",
        "    layerq = quantized_model.layer1[0]\n",
        "    Y = layer(X)\n",
        "    Yq = layerq(Xq)\n",
        "    diff_Y = Y-Yq\n",
        "    print(f\"diff_Y.std: {diff_Y.std()}\")\n",
        "    print(f\"diff_Y.rms_mean: {torch.mean(torch.sqrt(diff_Y**2))}\")\n",
        "    print(f\"diff_Y.max: {diff_Y.max()}\")\n",
        "    bn = layer.bn1\n",
        "    bn.eval()\n",
        "    Z = bn(X)\n",
        "    ZZ = X.clone()\n",
        "    for idx in range(X.shape[1]):\n",
        "        ZZ[0,idx,:,:] -= bn.running_mean[idx]\n",
        "        ZZ[0,idx,:,:] /= torch.sqrt(bn.running_var[idx]+bn.eps)\n",
        "        ZZ[0,idx,:,:] *= bn.weight[idx]\n",
        "        ZZ[0,idx,:,:] += bn.bias[idx]\n",
        "    # ZZ = (X-bn.running_mean.view(1,-1,1,1))/torch.sqrt(bn.running_var.view(1,-1,1,1)+bn.eps)\n",
        "    # ZZ = ZZ * bn.weight.view(1,-1,1,1) + bn.bias.view(1,-1,1,1)\n",
        "    diff_Z = Z-ZZ\n",
        "    print(f\"diff_Z.std: {diff_Z.std()}\")\n",
        "    print(f\"diff_Z.rms_mean: {torch.mean(torch.sqrt(diff_Z**2))}\")\n",
        "    print(f\"diff_Z.max: {diff_Z.max()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWirue0jTv3x",
        "outputId": "72ea3155-e06e-4c05-8a7a-104ecccc792f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: tensor([-5.0000, -4.9000, -4.8000, -4.7000, -4.6000, -4.5000, -4.4000, -4.3000,\n",
            "        -4.2000, -4.1000, -4.0000, -3.9000, -3.8000, -3.7000, -3.6000, -3.5000,\n",
            "        -3.4000, -3.3000, -3.2000, -3.1000, -3.0000, -2.9000, -2.8000, -2.7000,\n",
            "        -2.6000, -2.5000, -2.4000, -2.3000, -2.2000, -2.1000, -2.0000, -1.9000,\n",
            "        -1.8000, -1.7000, -1.6000, -1.5000, -1.4000, -1.3000, -1.2000, -1.1000,\n",
            "        -1.0000, -0.9000, -0.8000, -0.7000, -0.6000, -0.5000, -0.4000, -0.3000,\n",
            "        -0.2000, -0.1000,  0.0000,  0.1000,  0.2000,  0.3000,  0.4000,  0.5000,\n",
            "         0.6000,  0.7000,  0.8000,  0.9000,  1.0000,  1.1000,  1.2000,  1.3000,\n",
            "         1.4000,  1.5000,  1.6000,  1.7000,  1.8000,  1.9000,  2.0000,  2.1000,\n",
            "         2.2000,  2.3000,  2.4000,  2.5000,  2.6000,  2.7000,  2.8000,  2.9000,\n",
            "         3.0000,  3.1000,  3.2000,  3.3000,  3.4000,  3.5000,  3.6000,  3.7000,\n",
            "         3.8000,  3.9000,  4.0000,  4.1000,  4.2000,  4.3000,  4.4000,  4.5000,\n",
            "         4.6000,  4.7000,  4.8000,  4.9000])\n",
            "aa: tensor([-2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000,\n",
            "        -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000,\n",
            "        -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000,\n",
            "        -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -2.0000, -1.9062,\n",
            "        -1.7969, -1.7031, -1.5938, -1.5000, -1.4062, -1.2969, -1.2031, -1.0938,\n",
            "        -1.0000, -0.9062, -0.7969, -0.7031, -0.5938, -0.5000, -0.4062, -0.2969,\n",
            "        -0.2031, -0.0938,  0.0000,  0.0938,  0.2031,  0.2969,  0.4062,  0.5000,\n",
            "         0.5938,  0.7031,  0.7969,  0.9062,  1.0000,  1.0938,  1.2031,  1.2969,\n",
            "         1.4062,  1.5000,  1.5938,  1.7031,  1.7969,  1.9062,  1.9844,  1.9844,\n",
            "         1.9844,  1.9844,  1.9844,  1.9844,  1.9844,  1.9844,  1.9844,  1.9844,\n",
            "         1.9844,  1.9844,  1.9844,  1.9844,  1.9844,  1.9844,  1.9844,  1.9844,\n",
            "         1.9844,  1.9844,  1.9844,  1.9844,  1.9844,  1.9844,  1.9844,  1.9844,\n",
            "         1.9844,  1.9844,  1.9844,  1.9844])\n",
            "diff_a.std: 1.3536460399627686\n",
            "diff_a.rms_mean: 0.9061875343322754\n",
            "X:  tensor([[ 0.1105,  0.8368,  0.7208,  ..., -0.2783, -0.4629, -0.6242],\n",
            "        [ 1.1370,  1.0742, -0.2992,  ..., -0.8962, -0.8537, -0.4378],\n",
            "        [-0.1498, -0.8669,  0.0944,  ...,  0.1654,  0.5450,  0.4581],\n",
            "        ...,\n",
            "        [-2.4625, -0.4874,  0.0218,  ..., -0.4303, -0.2458, -0.0876],\n",
            "        [ 0.2466,  1.9611, -0.1878,  ...,  0.1062, -0.6380, -0.4935],\n",
            "        [ 1.1497,  0.4806, -1.1581,  ..., -0.4328, -0.5632,  0.6363]],\n",
            "       device='cuda:0')\n",
            "Xq:  tensor([[ 0.1128,  0.8760,  0.7212,  ..., -0.2505, -0.4321, -0.6382],\n",
            "        [ 1.1133,  1.0757, -0.3330,  ..., -0.8950, -0.9072, -0.4629],\n",
            "        [-0.1460, -0.8267,  0.1270,  ...,  0.1733,  0.5522,  0.4961],\n",
            "        ...,\n",
            "        [-2.4463, -0.4946, -0.0078,  ..., -0.4277, -0.2495, -0.0708],\n",
            "        [ 0.2500,  1.9741, -0.1782,  ...,  0.1064, -0.6675, -0.5044],\n",
            "        [ 1.1304,  0.4795, -1.1689,  ..., -0.4492, -0.5757,  0.6597]],\n",
            "       device='cuda:0')\n",
            "diff_X.std: 0.020448647439479828\n",
            "diff_X.rms_mean: 0.015014436095952988\n",
            "diff_X.max: 0.11430549621582031\n",
            "diff_Y.std: 0.10816899687051773\n",
            "diff_Y.rms_mean: 0.05670614540576935\n",
            "diff_Y.max: 2.054058074951172\n",
            "diff_Z.std: 4.038987810872641e-08\n",
            "diff_Z.rms_mean: 2.2484492490093544e-08\n",
            "diff_Z.max: 3.5762786865234375e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Loop"
      ],
      "metadata": {
        "id": "cRfJ5cOtA6PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing parameters with zeroes\n",
        "total_train = torch.zeros(num_epochs)\n",
        "correct_train = torch.zeros(num_epochs)\n",
        "avg_loss_train = torch.zeros(num_epochs)\n",
        "accuracy_train = torch.zeros(num_epochs)\n",
        "\n",
        "# TRAINING LOOP\n",
        "print(\"START TRAINING........\")\n",
        "train_losses = [] # store training loss for each batch\n",
        "train_accuracies = [] # store training accuracy for each batch\n",
        "val_accuracies = [] #store validation accuracy after each epoch\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  quantized_model.train() # Set the model to training mode\n",
        "  batch_losses = []\n",
        "  batch_accuracies = []\n",
        "\n",
        "  for input, target in train_loader:\n",
        "      input, target = input.to(device), target.to(device)\n",
        "\n",
        "      # forward\n",
        "      output = quantized_model(input)\n",
        "      loss = criterion(output, target)\n",
        "\n",
        "      # backward\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # *** Add gradient clipping here ***\n",
        "      torch.nn.utils.clip_grad_norm_(quantized_model.parameters(), max_norm=1)\n",
        "\n",
        "      # save data\n",
        "      batch_losses.append(loss.item())\n",
        "      _, predicted = output.max(1)\n",
        "      total = target.size(0)\n",
        "      correct = predicted.eq(target).sum().item()\n",
        "      batch_accuracies.append(100. * correct / total)\n",
        "\n",
        "  train_losses.append(batch_losses)\n",
        "  train_accuracies.append(batch_accuracies)\n",
        "  avg_loss_train[epoch] = np.mean(batch_losses)\n",
        "  accuracy_train[epoch] = np.mean(batch_accuracies)\n",
        "\n",
        "  # Validation after each epoch\n",
        "  val_accuracy = evaluate(quantized_model, test_loader, device)\n",
        "  val_accuracies.append(val_accuracy)\n",
        "\n",
        "  checkpoint_path = '/content/drive/My Drive/Colab Notebooks/checkpoints/quantized_checkpoint.pth'\n",
        "  if (epoch + 1) % 2 == 1:\n",
        "        save_checkpoint(quantized_model, optimizer, epoch, checkpoint_path)\n",
        "  print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
        "        f\"Train Loss: {avg_loss_train[epoch]:.4f} - \"\n",
        "        f\"Train Accuracy: {accuracy_train[epoch]:.2f}% - \"\n",
        "        f\"Validation Accuracy: {val_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjdiR2gNUs_D",
        "outputId": "9a45f549-b677-486f-e1c1-a18752283668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START TRAINING........\n",
            "Epoch [1/6] - Train Loss: 0.3345 - Train Accuracy: 92.63% - Validation Accuracy: 72.57%\n",
            "Epoch [2/6] - Train Loss: 0.3351 - Train Accuracy: 92.75% - Validation Accuracy: 72.98%\n",
            "Epoch [3/6] - Train Loss: 0.3382 - Train Accuracy: 92.45% - Validation Accuracy: 72.33%\n",
            "Epoch [4/6] - Train Loss: 0.3309 - Train Accuracy: 92.74% - Validation Accuracy: 72.59%\n",
            "Epoch [5/6] - Train Loss: 0.3350 - Train Accuracy: 92.60% - Validation Accuracy: 72.46%\n",
            "Epoch [6/6] - Train Loss: 0.3345 - Train Accuracy: 92.72% - Validation Accuracy: 72.66%\n"
          ]
        }
      ]
    }
  ]
}