{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyNYJ/pExFf5SO7i6p0EyPJ0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NRo0D5c01BmU","executionInfo":{"status":"ok","timestamp":1734358965755,"user_tz":-480,"elapsed":26058,"user":{"displayName":"Hung-Ting Tsai","userId":"08907122957587206864"}},"outputId":"261cee14-5366-4ec2-f74c-f1fb0cb8177a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"51txI3Y00dmM","executionInfo":{"status":"ok","timestamp":1734358976009,"user_tz":-480,"elapsed":8156,"user":{"displayName":"Hung-Ting Tsai","userId":"08907122957587206864"}},"outputId":"faa4dd57-ddd8-49da-b644-25e2b7e4162d"},"outputs":[{"output_type":"stream","name":"stdout","text":["GPU is available and being used.\n","Using device: cuda\n"]}],"source":["from torchvision.models import resnet18, ResNet18_Weights\n","import random\n","import copy\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import numpy as np\n","import torch\n","import torch.optim as optim\n","import torchvision\n","from torchvision.transforms import Resize\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","import torch.quantization\n","from torch.utils.data import random_split\n","from torch.utils.data import DataLoader\n","import pandas as pd\n","import time\n","\n","\n","# use GPU if available\n","if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","        print(\"GPU is available and being used.\")\n","else:\n","        device = torch.device(\"cpu\")\n","        print(\"GPU is not available, using CPU instead.\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")"]},{"cell_type":"code","source":["# Hyperparameters:\n","learning_rate = 0.001\n","momentum = 0.9\n","weight_decay = 1e-3\n","\n","num_epochs = 11\n","T_max = num_epochs\n","eta_min = 1e-5\n"],"metadata":{"id":"zaVFAHAq3fS_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define transformations for CIFAR-100 dataset\n","transform_train = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    transforms.RandomRotation(15),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","# Download the CIFAR-100 training dataset\n","download_train_dataset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n","download_test_dataset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n","\n","batch_size = 64\n","# Create DataLoader for training and validation datasets\n","train_loader = DataLoader(download_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","test_loader = DataLoader(download_test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0VIrA3dq1LIH","executionInfo":{"status":"ok","timestamp":1734359009030,"user_tz":-480,"elapsed":18920,"user":{"displayName":"Hung-Ting Tsai","userId":"08907122957587206864"}},"outputId":"8e083dc6-c0f7-4c5f-bba5-55ecda7667b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 169M/169M [00:13<00:00, 13.0MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-100-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["### Basic block and RestNet18 architecture"],"metadata":{"id":"Ug7Kcivp_2tS"}},{"cell_type":"code","source":["class BasicBlock(nn.Module):\n","    expansion = 1  # No expansion in BasicBlock\n","\n","    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, downsample=None):\n","        super(BasicBlock, self).__init__()\n","        self.stride = stride\n","\n","        # First convolutional layer\n","        self.conv1 = nn.Conv2d(\n","            in_channels, out_channels,\n","            kernel_size=kernel_size, stride=stride, padding=padding, bias=False\n","        )\n","\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        # Second convolutional layer\n","        self.conv2 = nn.Conv2d(\n","            out_channels, out_channels,\n","            kernel_size=kernel_size, stride=1, padding=padding, bias=False\n","        )\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","\n","        # Downsample layer for shortcut connection (if needed)\n","        self.downsample = downsample\n","\n","    def forward(self, x):\n","        identity = x  # Save the input tensor for the shortcut\n","\n","        # First layer\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        # Second layer\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        # Apply downsampling to the identity if necessary\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        # Add the identity (shortcut connection)\n","        out += identity\n","        out = self.relu(out)\n","\n","        return out"],"metadata":{"id":"N9XtbMR00io-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ResNet18(nn.Module):\n","    def __init__(self, num_classes=1000):\n","        super(ResNet18, self).__init__()\n","\n","        # Initial Convolution and Max Pool\n","        self.conv1 = nn.Conv2d(\n","            in_channels=3, out_channels=64,\n","            kernel_size=7, stride=2, padding=3, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        # Define layers using your BasicBlock\n","        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n","        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n","        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n","        self.layer4 = self._make_layer(256, 512, 2, stride=2)\n","\n","\n","        # Adaptive Average Pooling\n","        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","\n","        # Fully connected layer\n","        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)\n","\n","        # Initialize weights\n","        self._initialize_weights()\n","\n","    def _make_layer(self, in_channels, out_channels, blocks, stride):\n","        downsample = None\n","        if stride != 1 or in_channels != out_channels:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","\n","        layers = []\n","        layers.append(BasicBlock(in_channels, out_channels, stride=stride, downsample=downsample))\n","        for _ in range(1, blocks):\n","            layers.append(BasicBlock(out_channels, out_channels))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc(x)\n","\n","        return x\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)"],"metadata":{"id":"MLr8Iy-w0kRn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Functions"],"metadata":{"id":"7NSRFHJq1m7W"}},{"cell_type":"code","source":["def load_checkpoint(model, optimizer, path):\n","    checkpoint = torch.load(path, map_location=device)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    epoch = checkpoint['epoch']\n","    model.to(device)\n","\n","    for state in optimizer.state.values():\n","        for k, v in state.items():\n","            if isinstance(v, torch.Tensor):\n","                state[k] = v.to(device)\n","    return model, optimizer, epoch\n"],"metadata":{"id":"VALy42uQ1T1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def save_checkpoint(model, optimizer, epoch, path):\n","    # Create the directory if it doesn't exist\n","    import os\n","    os.makedirs(os.path.dirname(path), exist_ok=True)\n","    torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","    }, path)"],"metadata":{"id":"RSJITlsE5JRR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, data_loader, device):\n","    model.eval()  # Set model to evaluation mode\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():  # Disable gradient calculation during evaluation\n","        for images, labels in data_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)  # Get predictions\n","            _, predicted = torch.max(outputs.data, 1)  # Get predicted class labels\n","\n","            total += labels.size(0)  # Update total number of samples\n","            correct += (predicted == labels).sum().item()  # Update number of correct predictions\n","\n","    accuracy = 100 * correct / total  # Calculate accuracy\n","    return accuracy"],"metadata":{"id":"pTuKlF135EV1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Option 1: Reload from pretrain model, fit to our model"],"metadata":{"id":"p3T4Zabq_Jpp"}},{"cell_type":"code","source":["model = ResNet18(num_classes=100)\n","for name, param in model.named_parameters():\n","    if param.requires_grad:\n","        print(name, param.data.shape, param.data.sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uv9JldZtzbB2","executionInfo":{"status":"ok","timestamp":1734366685856,"user_tz":-480,"elapsed":879,"user":{"displayName":"Hung-Ting Tsai","userId":"08907122957587206864"}},"outputId":"8261eddc-70cc-4847-ad05-b8c8ad445941"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["conv1.weight torch.Size([64, 3, 7, 7]) tensor(2.8040)\n","bn1.weight torch.Size([64]) tensor(64.)\n","bn1.bias torch.Size([64]) tensor(0.)\n","layer1.0.conv1.weight torch.Size([64, 64, 3, 3]) tensor(10.2015)\n","layer1.0.bn1.weight torch.Size([64]) tensor(64.)\n","layer1.0.bn1.bias torch.Size([64]) tensor(0.)\n","layer1.0.conv2.weight torch.Size([64, 64, 3, 3]) tensor(-1.0695)\n","layer1.0.bn2.weight torch.Size([64]) tensor(64.)\n","layer1.0.bn2.bias torch.Size([64]) tensor(0.)\n","layer1.1.conv1.weight torch.Size([64, 64, 3, 3]) tensor(-7.3693)\n","layer1.1.bn1.weight torch.Size([64]) tensor(64.)\n","layer1.1.bn1.bias torch.Size([64]) tensor(0.)\n","layer1.1.conv2.weight torch.Size([64, 64, 3, 3]) tensor(-6.4864)\n","layer1.1.bn2.weight torch.Size([64]) tensor(64.)\n","layer1.1.bn2.bias torch.Size([64]) tensor(0.)\n","layer2.0.conv1.weight torch.Size([128, 64, 3, 3]) tensor(-21.4533)\n","layer2.0.bn1.weight torch.Size([128]) tensor(128.)\n","layer2.0.bn1.bias torch.Size([128]) tensor(0.)\n","layer2.0.conv2.weight torch.Size([128, 128, 3, 3]) tensor(16.4515)\n","layer2.0.bn2.weight torch.Size([128]) tensor(128.)\n","layer2.0.bn2.bias torch.Size([128]) tensor(0.)\n","layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1]) tensor(-3.9129)\n","layer2.0.downsample.1.weight torch.Size([128]) tensor(128.)\n","layer2.0.downsample.1.bias torch.Size([128]) tensor(0.)\n","layer2.1.conv1.weight torch.Size([128, 128, 3, 3]) tensor(-14.7744)\n","layer2.1.bn1.weight torch.Size([128]) tensor(128.)\n","layer2.1.bn1.bias torch.Size([128]) tensor(0.)\n","layer2.1.conv2.weight torch.Size([128, 128, 3, 3]) tensor(-25.2262)\n","layer2.1.bn2.weight torch.Size([128]) tensor(128.)\n","layer2.1.bn2.bias torch.Size([128]) tensor(0.)\n","layer3.0.conv1.weight torch.Size([256, 128, 3, 3]) tensor(2.8661)\n","layer3.0.bn1.weight torch.Size([256]) tensor(256.)\n","layer3.0.bn1.bias torch.Size([256]) tensor(0.)\n","layer3.0.conv2.weight torch.Size([256, 256, 3, 3]) tensor(-32.9669)\n","layer3.0.bn2.weight torch.Size([256]) tensor(256.)\n","layer3.0.bn2.bias torch.Size([256]) tensor(0.)\n","layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1]) tensor(42.8925)\n","layer3.0.downsample.1.weight torch.Size([256]) tensor(256.)\n","layer3.0.downsample.1.bias torch.Size([256]) tensor(0.)\n","layer3.1.conv1.weight torch.Size([256, 256, 3, 3]) tensor(32.7487)\n","layer3.1.bn1.weight torch.Size([256]) tensor(256.)\n","layer3.1.bn1.bias torch.Size([256]) tensor(0.)\n","layer3.1.conv2.weight torch.Size([256, 256, 3, 3]) tensor(7.9230)\n","layer3.1.bn2.weight torch.Size([256]) tensor(256.)\n","layer3.1.bn2.bias torch.Size([256]) tensor(0.)\n","layer4.0.conv1.weight torch.Size([512, 256, 3, 3]) tensor(-17.9200)\n","layer4.0.bn1.weight torch.Size([512]) tensor(512.)\n","layer4.0.bn1.bias torch.Size([512]) tensor(0.)\n","layer4.0.conv2.weight torch.Size([512, 512, 3, 3]) tensor(6.5584)\n","layer4.0.bn2.weight torch.Size([512]) tensor(512.)\n","layer4.0.bn2.bias torch.Size([512]) tensor(0.)\n","layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1]) tensor(1.5826)\n","layer4.0.downsample.1.weight torch.Size([512]) tensor(512.)\n","layer4.0.downsample.1.bias torch.Size([512]) tensor(0.)\n","layer4.1.conv1.weight torch.Size([512, 512, 3, 3]) tensor(-39.3875)\n","layer4.1.bn1.weight torch.Size([512]) tensor(512.)\n","layer4.1.bn1.bias torch.Size([512]) tensor(0.)\n","layer4.1.conv2.weight torch.Size([512, 512, 3, 3]) tensor(-7.5025)\n","layer4.1.bn2.weight torch.Size([512]) tensor(512.)\n","layer4.1.bn2.bias torch.Size([512]) tensor(0.)\n","fc.weight torch.Size([100, 512]) tensor(-4.3209)\n","fc.bias torch.Size([100]) tensor(0.0775)\n"]}]},{"cell_type":"code","source":["# Load the checkpoint\n","path = '/content/drive/My Drive/Colab Notebooks/checkpoints/pretrain_resnet18.pth'\n","checkpoint = torch.load(path, map_location=device)\n","pretrained_dict = checkpoint['model_state_dict']  # Load pretrained weights\n","model_dict = model.state_dict()\n","\n","# Filter out unnecessary keys from the pretrained state dict\n","filtered_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].size() == v.size()}\n","\n","# Update the current model's state dict with the filtered state dict\n","model_dict.update(filtered_dict)\n","\n","# Load the updated state dict back into the model\n","model.load_state_dict(model_dict)\n","\n","start_epoch = 0\n","model.to(device)\n","model.eval()\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n","\n","\n","print(\"start epoch: \", start_epoch)\n","for name, param in model.named_parameters():\n","    if param.requires_grad:\n","        print(name, param.data.shape, param.data.sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IGGN6Y645rhp","executionInfo":{"status":"ok","timestamp":1734367357541,"user_tz":-480,"elapsed":747,"user":{"displayName":"Hung-Ting Tsai","userId":"08907122957587206864"}},"outputId":"5df59dcd-db8f-478b-eb94-a80bd553e673"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-51-cf69576fea25>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(path, map_location=device)\n"]},{"output_type":"stream","name":"stdout","text":["start epoch:  0\n","conv1.weight torch.Size([64, 3, 7, 7]) tensor(-0.0831, device='cuda:0')\n","bn1.weight torch.Size([64]) tensor(15.1651, device='cuda:0')\n","bn1.bias torch.Size([64]) tensor(10.5746, device='cuda:0')\n","layer1.0.conv1.weight torch.Size([64, 64, 3, 3]) tensor(-252.3925, device='cuda:0')\n","layer1.0.bn1.weight torch.Size([64]) tensor(20.1336, device='cuda:0')\n","layer1.0.bn1.bias torch.Size([64]) tensor(-1.2524, device='cuda:0')\n","layer1.0.conv2.weight torch.Size([64, 64, 3, 3]) tensor(-79.8547, device='cuda:0')\n","layer1.0.bn2.weight torch.Size([64]) tensor(18.7809, device='cuda:0')\n","layer1.0.bn2.bias torch.Size([64]) tensor(-1.7764, device='cuda:0')\n","layer1.1.conv1.weight torch.Size([64, 64, 3, 3]) tensor(-170.1751, device='cuda:0')\n","layer1.1.bn1.weight torch.Size([64]) tensor(19.5954, device='cuda:0')\n","layer1.1.bn1.bias torch.Size([64]) tensor(-5.7394, device='cuda:0')\n","layer1.1.conv2.weight torch.Size([64, 64, 3, 3]) tensor(-129.5225, device='cuda:0')\n","layer1.1.bn2.weight torch.Size([64]) tensor(24.7789, device='cuda:0')\n","layer1.1.bn2.bias torch.Size([64]) tensor(-3.5377, device='cuda:0')\n","layer2.0.conv1.weight torch.Size([128, 64, 3, 3]) tensor(-375.3674, device='cuda:0')\n","layer2.0.bn1.weight torch.Size([128]) tensor(36.0362, device='cuda:0')\n","layer2.0.bn1.bias torch.Size([128]) tensor(-16.7661, device='cuda:0')\n","layer2.0.conv2.weight torch.Size([128, 128, 3, 3]) tensor(-586.1251, device='cuda:0')\n","layer2.0.bn2.weight torch.Size([128]) tensor(39.7254, device='cuda:0')\n","layer2.0.bn2.bias torch.Size([128]) tensor(-3.9100, device='cuda:0')\n","layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1]) tensor(-56.4978, device='cuda:0')\n","layer2.0.downsample.1.weight torch.Size([128]) tensor(22.5557, device='cuda:0')\n","layer2.0.downsample.1.bias torch.Size([128]) tensor(-3.9100, device='cuda:0')\n","layer2.1.conv1.weight torch.Size([128, 128, 3, 3]) tensor(-623.6708, device='cuda:0')\n","layer2.1.bn1.weight torch.Size([128]) tensor(38.3533, device='cuda:0')\n","layer2.1.bn1.bias torch.Size([128]) tensor(-33.6367, device='cuda:0')\n","layer2.1.conv2.weight torch.Size([128, 128, 3, 3]) tensor(-791.1605, device='cuda:0')\n","layer2.1.bn2.weight torch.Size([128]) tensor(33.9417, device='cuda:0')\n","layer2.1.bn2.bias torch.Size([128]) tensor(-25.0653, device='cuda:0')\n","layer3.0.conv1.weight torch.Size([256, 128, 3, 3]) tensor(-1289.0010, device='cuda:0')\n","layer3.0.bn1.weight torch.Size([256]) tensor(75.5351, device='cuda:0')\n","layer3.0.bn1.bias torch.Size([256]) tensor(-42.3964, device='cuda:0')\n","layer3.0.conv2.weight torch.Size([256, 256, 3, 3]) tensor(-1717.4915, device='cuda:0')\n","layer3.0.bn2.weight torch.Size([256]) tensor(81.6151, device='cuda:0')\n","layer3.0.bn2.bias torch.Size([256]) tensor(-17.7448, device='cuda:0')\n","layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1]) tensor(-170.4871, device='cuda:0')\n","layer3.0.downsample.1.weight torch.Size([256]) tensor(23.9458, device='cuda:0')\n","layer3.0.downsample.1.bias torch.Size([256]) tensor(-17.7448, device='cuda:0')\n","layer3.1.conv1.weight torch.Size([256, 256, 3, 3]) tensor(-3014.5835, device='cuda:0')\n","layer3.1.bn1.weight torch.Size([256]) tensor(66.0899, device='cuda:0')\n","layer3.1.bn1.bias torch.Size([256]) tensor(-73.1897, device='cuda:0')\n","layer3.1.conv2.weight torch.Size([256, 256, 3, 3]) tensor(-3478.6147, device='cuda:0')\n","layer3.1.bn2.weight torch.Size([256]) tensor(55.1559, device='cuda:0')\n","layer3.1.bn2.bias torch.Size([256]) tensor(-52.8234, device='cuda:0')\n","layer4.0.conv1.weight torch.Size([512, 256, 3, 3]) tensor(-5476.9194, device='cuda:0')\n","layer4.0.bn1.weight torch.Size([512]) tensor(127.7537, device='cuda:0')\n","layer4.0.bn1.bias torch.Size([512]) tensor(-138.2961, device='cuda:0')\n","layer4.0.conv2.weight torch.Size([512, 512, 3, 3]) tensor(-11898.0508, device='cuda:0')\n","layer4.0.bn2.weight torch.Size([512]) tensor(206.8254, device='cuda:0')\n","layer4.0.bn2.bias torch.Size([512]) tensor(-130.9835, device='cuda:0')\n","layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1]) tensor(-595.4565, device='cuda:0')\n","layer4.0.downsample.1.weight torch.Size([512]) tensor(119.0379, device='cuda:0')\n","layer4.0.downsample.1.bias torch.Size([512]) tensor(-130.9835, device='cuda:0')\n","layer4.1.conv1.weight torch.Size([512, 512, 3, 3]) tensor(-20918.8828, device='cuda:0')\n","layer4.1.bn1.weight torch.Size([512]) tensor(142.3601, device='cuda:0')\n","layer4.1.bn1.bias torch.Size([512]) tensor(-143.3806, device='cuda:0')\n","layer4.1.conv2.weight torch.Size([512, 512, 3, 3]) tensor(-3570.6172, device='cuda:0')\n","layer4.1.bn2.weight torch.Size([512]) tensor(1055.0254, device='cuda:0')\n","layer4.1.bn2.bias torch.Size([512]) tensor(141.3846, device='cuda:0')\n","fc.weight torch.Size([100, 512]) tensor(-947.0810, device='cuda:0')\n","fc.bias torch.Size([100]) tensor(-0.1404, device='cuda:0')\n"]}]},{"cell_type":"code","source":["# Identify and print unused and uninitialized parameters\n","unused_keys = [k for k in pretrained_dict if k not in filtered_dict]\n","print(\"Unused keys from the checkpoint:\", unused_keys)\n","\n","uninitialized_keys = [k for k in model_dict if k not in filtered_dict]\n","print(\"Keys in the new model not initialized from checkpoint:\", uninitialized_keys)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ivFwJQQAVoLo","executionInfo":{"status":"ok","timestamp":1734367453268,"user_tz":-480,"elapsed":892,"user":{"displayName":"Hung-Ting Tsai","userId":"08907122957587206864"}},"outputId":"af5be0c5-c54e-453d-a2cd-e98860051b7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Unused keys from the checkpoint: []\n","Keys in the new model not initialized from checkpoint: []\n"]}]},{"cell_type":"markdown","source":["### Option 2: Keep training with our own model checkpoint"],"metadata":{"id":"fwoArtc1_ZkR"}},{"cell_type":"code","source":["# Example usage before resuming training\n","checkpoint_path = '/content/drive/My Drive/Colab Notebooks/checkpoints/transfer_learning_checkpoint.pth'\n","model = ResNet18(num_classes=100)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","\n","model, optimizer, start_epoch = load_checkpoint(model, optimizer, checkpoint_path)\n","model.to(device)\n","\n","# Define loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","\n","# Create model, schedueler\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n","\n","# # Freeze all layers first\n","# for param in model.parameters():\n","#     param.requires_grad = False\n","\n","# # Unfreeze the deeper layers and fully connected layer for fine-tuning\n","# for param in model.layer3.parameters():\n","#     param.requires_grad = True\n","# for param in model.layer4.parameters():\n","#     param.requires_grad = True\n","# for param in model.fc.parameters():\n","#     param.requires_grad = True\n","\n","# Verify the model\n","print(\"start epoch: \", start_epoch)\n","for name, param in model.named_parameters():\n","    if param.requires_grad:\n","        print(name, param.data.shape, param.data.sum())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wSZrWiw-98aX","executionInfo":{"status":"ok","timestamp":1734368491701,"user_tz":-480,"elapsed":593,"user":{"displayName":"Hung-Ting Tsai","userId":"08907122957587206864"}},"outputId":"fabfed8b-be90-4130-c300-18816bbaa00e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-7-919c0df432fe>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint = torch.load(path, map_location=device)\n"]},{"output_type":"stream","name":"stdout","text":["start epoch:  4\n","conv1.weight torch.Size([64, 3, 7, 7]) tensor(5.1260, device='cuda:0')\n","bn1.weight torch.Size([64]) tensor(12.0971, device='cuda:0')\n","bn1.bias torch.Size([64]) tensor(6.9236, device='cuda:0')\n","layer1.0.conv1.weight torch.Size([64, 64, 3, 3]) tensor(-83.6823, device='cuda:0')\n","layer1.0.bn1.weight torch.Size([64]) tensor(15.5185, device='cuda:0')\n","layer1.0.bn1.bias torch.Size([64]) tensor(-2.6118, device='cuda:0')\n","layer1.0.conv2.weight torch.Size([64, 64, 3, 3]) tensor(-11.2146, device='cuda:0')\n","layer1.0.bn2.weight torch.Size([64]) tensor(14.6549, device='cuda:0')\n","layer1.0.bn2.bias torch.Size([64]) tensor(-3.4231, device='cuda:0')\n","layer1.1.conv1.weight torch.Size([64, 64, 3, 3]) tensor(-41.3265, device='cuda:0')\n","layer1.1.bn1.weight torch.Size([64]) tensor(15.5958, device='cuda:0')\n","layer1.1.bn1.bias torch.Size([64]) tensor(-5.0180, device='cuda:0')\n","layer1.1.conv2.weight torch.Size([64, 64, 3, 3]) tensor(-53.1163, device='cuda:0')\n","layer1.1.bn2.weight torch.Size([64]) tensor(19.4880, device='cuda:0')\n","layer1.1.bn2.bias torch.Size([64]) tensor(-3.0798, device='cuda:0')\n","layer2.0.conv1.weight torch.Size([128, 64, 3, 3]) tensor(-122.7320, device='cuda:0')\n","layer2.0.bn1.weight torch.Size([128]) tensor(27.8558, device='cuda:0')\n","layer2.0.bn1.bias torch.Size([128]) tensor(-14.1763, device='cuda:0')\n","layer2.0.conv2.weight torch.Size([128, 128, 3, 3]) tensor(-102.2792, device='cuda:0')\n","layer2.0.bn2.weight torch.Size([128]) tensor(31.7517, device='cuda:0')\n","layer2.0.bn2.bias torch.Size([128]) tensor(-4.0077, device='cuda:0')\n","layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1]) tensor(-21.4946, device='cuda:0')\n","layer2.0.downsample.1.weight torch.Size([128]) tensor(18.0793, device='cuda:0')\n","layer2.0.downsample.1.bias torch.Size([128]) tensor(-4.0077, device='cuda:0')\n","layer2.1.conv1.weight torch.Size([128, 128, 3, 3]) tensor(-130.1724, device='cuda:0')\n","layer2.1.bn1.weight torch.Size([128]) tensor(31.5915, device='cuda:0')\n","layer2.1.bn1.bias torch.Size([128]) tensor(-30.4933, device='cuda:0')\n","layer2.1.conv2.weight torch.Size([128, 128, 3, 3]) tensor(-156.9373, device='cuda:0')\n","layer2.1.bn2.weight torch.Size([128]) tensor(26.3701, device='cuda:0')\n","layer2.1.bn2.bias torch.Size([128]) tensor(-20.6882, device='cuda:0')\n","layer3.0.conv1.weight torch.Size([256, 128, 3, 3]) tensor(-334.0380, device='cuda:0')\n","layer3.0.bn1.weight torch.Size([256]) tensor(60.5941, device='cuda:0')\n","layer3.0.bn1.bias torch.Size([256]) tensor(-39.3981, device='cuda:0')\n","layer3.0.conv2.weight torch.Size([256, 256, 3, 3]) tensor(-160.6460, device='cuda:0')\n","layer3.0.bn2.weight torch.Size([256]) tensor(71.4361, device='cuda:0')\n","layer3.0.bn2.bias torch.Size([256]) tensor(-18.1055, device='cuda:0')\n","layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1]) tensor(-42.5289, device='cuda:0')\n","layer3.0.downsample.1.weight torch.Size([256]) tensor(23.3303, device='cuda:0')\n","layer3.0.downsample.1.bias torch.Size([256]) tensor(-18.1055, device='cuda:0')\n","layer3.1.conv1.weight torch.Size([256, 256, 3, 3]) tensor(-215.9586, device='cuda:0')\n","layer3.1.bn1.weight torch.Size([256]) tensor(51.9877, device='cuda:0')\n","layer3.1.bn1.bias torch.Size([256]) tensor(-62.8712, device='cuda:0')\n","layer3.1.conv2.weight torch.Size([256, 256, 3, 3]) tensor(-558.4124, device='cuda:0')\n","layer3.1.bn2.weight torch.Size([256]) tensor(39.3424, device='cuda:0')\n","layer3.1.bn2.bias torch.Size([256]) tensor(-44.1558, device='cuda:0')\n","layer4.0.conv1.weight torch.Size([512, 256, 3, 3]) tensor(-430.7116, device='cuda:0')\n","layer4.0.bn1.weight torch.Size([512]) tensor(105.3152, device='cuda:0')\n","layer4.0.bn1.bias torch.Size([512]) tensor(-124.3190, device='cuda:0')\n","layer4.0.conv2.weight torch.Size([512, 512, 3, 3]) tensor(-1288.4487, device='cuda:0')\n","layer4.0.bn2.weight torch.Size([512]) tensor(156.6272, device='cuda:0')\n","layer4.0.bn2.bias torch.Size([512]) tensor(-108.0253, device='cuda:0')\n","layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1]) tensor(-63.7689, device='cuda:0')\n","layer4.0.downsample.1.weight torch.Size([512]) tensor(103.4014, device='cuda:0')\n","layer4.0.downsample.1.bias torch.Size([512]) tensor(-108.0253, device='cuda:0')\n","layer4.1.conv1.weight torch.Size([512, 512, 3, 3]) tensor(-3049.0293, device='cuda:0')\n","layer4.1.bn1.weight torch.Size([512]) tensor(116.8271, device='cuda:0')\n","layer4.1.bn1.bias torch.Size([512]) tensor(-127.3885, device='cuda:0')\n","layer4.1.conv2.weight torch.Size([512, 512, 3, 3]) tensor(-494.3585, device='cuda:0')\n","layer4.1.bn2.weight torch.Size([512]) tensor(502.8737, device='cuda:0')\n","layer4.1.bn2.bias torch.Size([512]) tensor(80.6758, device='cuda:0')\n","fc.weight torch.Size([100, 512]) tensor(-795.5214, device='cuda:0')\n","fc.bias torch.Size([100]) tensor(-0.2705, device='cuda:0')\n"]}]},{"cell_type":"code","source":["# Initializing parameters with zeroes\n","total_train = torch.zeros(num_epochs)\n","correct_train = torch.zeros(num_epochs)\n","avg_loss_train = torch.zeros(num_epochs)\n","accuracy_train = torch.zeros(num_epochs)\n","\n","# TRAINING LOOP\n","print(\"START TRAINING........\")\n","train_losses = [] # store training loss for each batch\n","train_accuracies = [] # store training accuracy for each batch\n","test_accuracies = [] #store test accuracy after each epoch\n","\n","for epoch in range(start_epoch, num_epochs):\n","  model.train() # Set the model to training mode\n","  batch_losses = []\n","  batch_accuracies = []\n","\n","  for input, target in train_loader:\n","      input, target = input.to(device), target.to(device)\n","\n","      # forward\n","      output = model(input)\n","      loss = criterion(output, target)\n","\n","      # backward\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","\n","      # *** Add gradient clipping here ***\n","      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","\n","\n","      # save data\n","      batch_losses.append(loss.item())\n","      _, predicted = output.max(1)\n","      total = target.size(0)\n","      correct = predicted.eq(target).sum().item()\n","      batch_accuracies.append(100. * correct / total)\n","\n","  train_losses.append(batch_losses) # append the batch losses for this epoch to the main list\n","  train_accuracies.append(batch_accuracies) # append the batch accuracies for this epoch to the main list\n","  avg_loss_train[epoch] = np.mean(batch_losses) # calculate and store average loss for the epoch\n","  accuracy_train[epoch] = np.mean(batch_accuracies) # calculate and store average accuracy for the epoch\n","\n","  #Validation after each epoch\n","  test_accuracy = evaluate(model, test_loader, device)\n","  test_accuracies.append(test_accuracy)\n","\n","  checkpoint_path = '/content/drive/My Drive/Colab Notebooks/checkpoints/transfer_learning_checkpoint.pth'\n","  if (epoch + 1) % 2 == 1:\n","        save_checkpoint(model, optimizer, epoch, checkpoint_path)\n","  print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n","        f\"Train Loss: {avg_loss_train[epoch]:.4f} - \"\n","        f\"Train Accuracy: {accuracy_train[epoch]:.2f}% - \"\n","        f\"Validation Accuracy: {test_accuracy:.2f}% \"\n","        )\n","\n","  scheduler.step()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5xG_Ovd85LhH","outputId":"d73f3db5-6393-4e9e-e0db-2b7411114d1b","executionInfo":{"status":"ok","timestamp":1734369816780,"user_tz":-480,"elapsed":1309035,"user":{"displayName":"Hung-Ting Tsai","userId":"08907122957587206864"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["START TRAINING........\n","Epoch [5/11] - Train Loss: 0.4893 - Train Accuracy: 86.29% - Validation Accuracy: 76.15% \n","Epoch [6/11] - Train Loss: 0.4291 - Train Accuracy: 88.26% - Validation Accuracy: 76.33% \n","Epoch [7/11] - Train Loss: 0.3749 - Train Accuracy: 89.93% - Validation Accuracy: 76.52% \n","Epoch [8/11] - Train Loss: 0.3272 - Train Accuracy: 91.53% - Validation Accuracy: 76.94% \n","Epoch [9/11] - Train Loss: 0.2703 - Train Accuracy: 93.32% - Validation Accuracy: 77.22% \n","Epoch [10/11] - Train Loss: 0.2273 - Train Accuracy: 94.77% - Validation Accuracy: 77.38% \n","Epoch [11/11] - Train Loss: 0.1875 - Train Accuracy: 95.89% - Validation Accuracy: 77.45% \n"]}]}]}